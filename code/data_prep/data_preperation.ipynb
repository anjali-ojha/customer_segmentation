{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78be37067ea828e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T22:33:48.093314Z",
     "start_time": "2023-11-11T22:33:47.501115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configs.py\n"
     ]
    }
   ],
   "source": [
    "%run /Users/hims/sjsu/bigdata_tech-228/project/customer_segmentation/code/storage/snowflake.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T22:34:21.825088Z",
     "start_time": "2023-11-11T22:34:21.811930Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType, StringType, MapType, ArrayType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# baseOutputPath = baseOutputPath + \"/full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf465d677ba4db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T22:34:23.014715Z",
     "start_time": "2023-11-11T22:34:22.365288Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m, in \u001B[0;36mprocess_business_data\u001B[0;34m(spark)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 3\u001B[0m     businessDf \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mbaseOutputPath\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/business\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:544\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    535\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n\u001B[1;32m    536\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    541\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n\u001B[1;32m    542\u001B[0m )\n\u001B[0;32m--> 544\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: file:/tmp/test-1/business.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m         businessDf \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbaseOutputPath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/business\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m businessDf\n\u001B[0;32m---> 29\u001B[0m business_df \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_business_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 24\u001B[0m, in \u001B[0;36mprocess_business_data\u001B[0;34m(spark)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m      6\u001B[0m     schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m      7\u001B[0m         StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maddress\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      8\u001B[0m         StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattributes\u001B[39m\u001B[38;5;124m\"\u001B[39m, MapType(StringType(), StringType()), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m         StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     21\u001B[0m     ])\n\u001B[1;32m     23\u001B[0m     businessDf \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbaseInputPath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/yelp_academic_dataset_business.json\u001B[39m\u001B[38;5;124m'\u001B[39m, schema) \\\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategories\u001B[39m\u001B[38;5;124m\"\u001B[39m, split(\u001B[43mcol\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategories\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     25\u001B[0m     businessDf\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbaseOutputPath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/business\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m     businessDf \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbaseOutputPath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/business\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "def process_business_data(spark):\n",
    "    try:\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"hours\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"is_open\", LongType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"review_count\", LongType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ])\n",
    "        \n",
    "        businessDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_business.json', schema) \\\n",
    "        .withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "        businessDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/business\")\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    return businessDf\n",
    "\n",
    "business_df = process_business_data(spark)\n",
    "# business_df.swho()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca71d457191f09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:17.525220Z",
     "start_time": "2023-11-11T12:25:17.402899Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_user_data(spark):\n",
    "    try:\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "    except Exception as e:\n",
    "        userDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .drop(\"friends\") \\\n",
    "            .withColumn(\"elite\", split(col(\"elite\"), \", \")) \\\n",
    "            .withColumn(\"yelping_since\", col(\"yelping_since\").cast(\"timestamp\"))\n",
    "        \n",
    "        userDf.printSchema()\n",
    "        userDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/user\")\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "        \n",
    "    return userDf\n",
    "\n",
    "user_df = process_user_data(spark)\n",
    "# user_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb936e37caffac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:27.229788Z",
     "start_time": "2023-11-11T12:25:27.150742Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_friends_data(spark):\n",
    "    try:\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    except Exception as e:\n",
    "        friendsDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .select(\"user_id\", split(col(\"friends\"), \", \").alias(\"friends\"))\n",
    "        \n",
    "        friendsDf.printSchema()\n",
    "        friendsDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/friends\")\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    \n",
    "    return friendsDf\n",
    "\n",
    "friends_df = process_friends_data(spark)\n",
    "# friends_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1637d8718a36c1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:39.498031Z",
     "start_time": "2023-11-11T12:25:39.409987Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_checkin_data(spark):\n",
    "    try:\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "    except Exception as e:\n",
    "        checkinDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_checkin.json') \\\n",
    "            .withColumn(\"date\", expr(\"transform(split(date, ', '), d -> to_timestamp(d))\").cast(ArrayType(TimestampType())))\n",
    "\n",
    "        checkinDf.printSchema()\n",
    "\n",
    "        checkinDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/checkin\")\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "\n",
    "    return checkinDf\n",
    "\n",
    "checkin_df = process_checkin_data(spark)\n",
    "# checkin_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef6e5e012af697a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:50.500668Z",
     "start_time": "2023-11-11T12:25:50.427689Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tip_data(spark):\n",
    "    try:\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    except Exception as e:\n",
    "        tipDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_tip.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "        \n",
    "        tipDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/tip\")\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    \n",
    "    return tipDf\n",
    "\n",
    "tip_df = process_tip_data(spark)\n",
    "# tip_df.show(4, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6256d6f660533e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:03.367490Z",
     "start_time": "2023-11-11T12:26:02.968180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hims/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def tokenize_and_get_top_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "    return top_words\n",
    "\n",
    "def process_review_data(spark):\n",
    "    try:\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "    except Exception as e:\n",
    "        reviewDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "            .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "        reviewDf.printSchema()\n",
    "        reviewDf.write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/review\")\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "        \n",
    "    return reviewDf\n",
    "\n",
    "review_df = process_review_data(spark)\n",
    "# review_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c07db030799225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:06.696872Z",
     "start_time": "2023-11-11T12:26:06.691625Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# \n",
    "# import nltk\n",
    "# \n",
    "# nltk.download('vader_lexicon')\n",
    "# \n",
    "# def get_sentiments_df(df):\n",
    "#     # Initialize the Sentiment Intensity Analyzer\n",
    "#     sia = SentimentIntensityAnalyzer()\n",
    "#     # Define a UDF for sentiment analysis\n",
    "#     def get_sentiment(text):\n",
    "#         sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "#         if sentiment_score >= 0.05:\n",
    "#             return \"positive\"\n",
    "#         elif sentiment_score <= -0.05:\n",
    "#             return \"negative\"\n",
    "#         else:\n",
    "#             return \"neutral\"\n",
    "#     \n",
    "#     sentiment_udf = udf(get_sentiment, StringType())\n",
    "#     df = review_df.select(\"user_id\", \"text\", sentiment_udf(col(\"text\")).alias(\"sentiment\"))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e1d3b6f534d712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:08.942513Z",
     "start_time": "2023-11-11T12:26:08.937555Z"
    }
   },
   "outputs": [],
   "source": [
    "# # from nltk.tokenize import word_tokenize\n",
    "# # from nltk.probability import FreqDist\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from pyspark.sql.types import StringType, ArrayType, IntegerType, MapType\n",
    "# # from pyspark.sql.functions import col, udf, concat_ws, collect_list\n",
    "# \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# \n",
    "# def get_frequent_words():\n",
    "# \n",
    "#     @udf(ArrayType(StringType()))\n",
    "#     # @udf(MapType(StringType(), IntegerType()))\n",
    "#     def tokenize_and_get_top_words(text):\n",
    "# \n",
    "#         tokens = word_tokenize(text)\n",
    "#         tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in stop_words]    \n",
    "#         freq_dist = FreqDist(tokens)\n",
    "#         top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "#         print(top_words)\n",
    "#         return top_words\n",
    "# \n",
    "#     df = review_df.sample(.0001).select(\"user_id\", \"text\") \\\n",
    "#         .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "# \n",
    "#     # .groupBy(\"user_id\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"texts\")) \\\n",
    "# \n",
    "#     return df\n",
    "# \n",
    "# get_frequent_words().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe9c2f99f3b7d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:12.545736Z",
     "start_time": "2023-11-11T12:26:12.536426Z"
    }
   },
   "outputs": [],
   "source": [
    "# review_df.sample(.0001).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c1bd61888af99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca406dc252d78c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c91529e7d09427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bb2b9b3d664da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed566c1bdeaa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8d42be0b06a7182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:48:36.734310Z",
     "start_time": "2023-11-11T08:48:36.552788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hims/Library/CloudStorage/GoogleDrive-anjalihimanshuojha@gmail.com/Other computers/My MacBook Air/sjsu/bigdata_tech-228/project/customer_segmentation/code/data_prep\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d326d27e4c7b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e1c16955dacb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:05:03.943551Z",
     "start_time": "2023-11-11T04:05:03.939759Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple_data = spark.sparkContext.parallelize([[1, \"Alice\", 50]]).toDF()\n",
    "# simple_data.count()\n",
    "# simple_data.first()\n",
    "# simple_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85234419e01b2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
