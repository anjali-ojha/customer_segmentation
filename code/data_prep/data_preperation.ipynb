{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78be37067ea828e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:06.533620Z",
     "start_time": "2023-11-11T12:25:01.654730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configs.py\n"
     ]
    }
   ],
   "source": [
    "%run /Users/hims/sjsu/bigdata_tech-228/project/customer_segmentation/code/storage/snowflake.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:06.534073Z",
     "start_time": "2023-11-11T12:25:06.516551Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "baseInputPath = \"/Users/hims/Downloads/yelp_dataset/\"\n",
    "baseOutputPath = \"/tmp/test-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf465d677ba4db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:14.298673Z",
     "start_time": "2023-11-11T12:25:12.270738Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_business_data(spark):\n",
    "    try:\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"hours\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"is_open\", LongType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"review_count\", LongType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ])\n",
    "        \n",
    "        businessDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_business.json', schema) \\\n",
    "        .withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "        businessDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/business\")\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    return businessDf\n",
    "\n",
    "business_df = process_business_data(spark)\n",
    "# business_df.swho()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca71d457191f09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:17.525220Z",
     "start_time": "2023-11-11T12:25:17.402899Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_user_data(spark):\n",
    "    try:\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "    except Exception as e:\n",
    "        userDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .drop(\"friends\") \\\n",
    "            .withColumn(\"elite\", split(col(\"elite\"), \", \")) \\\n",
    "            .withColumn(\"yelping_since\", col(\"yelping_since\").cast(\"timestamp\"))\n",
    "        \n",
    "        userDf.printSchema()\n",
    "        userDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/user\")\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "        \n",
    "    return userDf\n",
    "\n",
    "user_df = process_user_data(spark)\n",
    "# user_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb936e37caffac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:27.229788Z",
     "start_time": "2023-11-11T12:25:27.150742Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_friends_data(spark):\n",
    "    try:\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    except Exception as e:\n",
    "        friendsDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .select(\"user_id\", split(col(\"friends\"), \", \").alias(\"friends\"))\n",
    "        \n",
    "        friendsDf.printSchema()\n",
    "        friendsDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/friends\")\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    \n",
    "    return friendsDf\n",
    "\n",
    "friends_df = process_friends_data(spark)\n",
    "# friends_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1637d8718a36c1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:39.498031Z",
     "start_time": "2023-11-11T12:25:39.409987Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_checkin_data(spark):\n",
    "    try:\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "    except Exception as e:\n",
    "        checkinDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_checkin.json') \\\n",
    "            .withColumn(\"date\", expr(\"transform(split(date, ', '), d -> to_timestamp(d))\").cast(ArrayType(TimestampType())))\n",
    "\n",
    "        checkinDf.printSchema()\n",
    "\n",
    "        checkinDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/checkin\")\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "\n",
    "    return checkinDf\n",
    "\n",
    "checkin_df = process_checkin_data(spark)\n",
    "# checkin_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef6e5e012af697a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:25:50.500668Z",
     "start_time": "2023-11-11T12:25:50.427689Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tip_data(spark):\n",
    "    try:\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    except Exception as e:\n",
    "        tipDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_tip.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "        \n",
    "        tipDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/tip\")\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    \n",
    "    return tipDf\n",
    "\n",
    "tip_df = process_tip_data(spark)\n",
    "# tip_df.show(4, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154ddb877eedf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446c4c1b593419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caea94eac02882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fde7358d09b82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d3807e75fae4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca10da0a09facc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723384fb3d0459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550c51a5bf5ba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6256d6f660533e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:03.367490Z",
     "start_time": "2023-11-11T12:26:02.968180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hims/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def tokenize_and_get_top_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "    return top_words\n",
    "\n",
    "def process_review_data(spark):\n",
    "    try:\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "    except Exception as e:\n",
    "        reviewDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "            .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "        reviewDf.printSchema()\n",
    "        reviewDf.write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/review\")\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "        \n",
    "    return reviewDf\n",
    "\n",
    "review_df = process_review_data(spark)\n",
    "# review_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c07db030799225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:06.696872Z",
     "start_time": "2023-11-11T12:26:06.691625Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# \n",
    "# import nltk\n",
    "# \n",
    "# nltk.download('vader_lexicon')\n",
    "# \n",
    "# def get_sentiments_df(df):\n",
    "#     # Initialize the Sentiment Intensity Analyzer\n",
    "#     sia = SentimentIntensityAnalyzer()\n",
    "#     # Define a UDF for sentiment analysis\n",
    "#     def get_sentiment(text):\n",
    "#         sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "#         if sentiment_score >= 0.05:\n",
    "#             return \"positive\"\n",
    "#         elif sentiment_score <= -0.05:\n",
    "#             return \"negative\"\n",
    "#         else:\n",
    "#             return \"neutral\"\n",
    "#     \n",
    "#     sentiment_udf = udf(get_sentiment, StringType())\n",
    "#     df = review_df.select(\"user_id\", \"text\", sentiment_udf(col(\"text\")).alias(\"sentiment\"))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e1d3b6f534d712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:08.942513Z",
     "start_time": "2023-11-11T12:26:08.937555Z"
    }
   },
   "outputs": [],
   "source": [
    "# # from nltk.tokenize import word_tokenize\n",
    "# # from nltk.probability import FreqDist\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from pyspark.sql.types import StringType, ArrayType, IntegerType, MapType\n",
    "# # from pyspark.sql.functions import col, udf, concat_ws, collect_list\n",
    "# \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# \n",
    "# def get_frequent_words():\n",
    "# \n",
    "#     @udf(ArrayType(StringType()))\n",
    "#     # @udf(MapType(StringType(), IntegerType()))\n",
    "#     def tokenize_and_get_top_words(text):\n",
    "# \n",
    "#         tokens = word_tokenize(text)\n",
    "#         tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in stop_words]    \n",
    "#         freq_dist = FreqDist(tokens)\n",
    "#         top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "#         print(top_words)\n",
    "#         return top_words\n",
    "# \n",
    "#     df = review_df.sample(.0001).select(\"user_id\", \"text\") \\\n",
    "#         .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "# \n",
    "#     # .groupBy(\"user_id\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"texts\")) \\\n",
    "# \n",
    "#     return df\n",
    "# \n",
    "# get_frequent_words().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe9c2f99f3b7d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:12.545736Z",
     "start_time": "2023-11-11T12:26:12.536426Z"
    }
   },
   "outputs": [],
   "source": [
    "# review_df.sample(.0001).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c1bd61888af99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca406dc252d78c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c91529e7d09427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bb2b9b3d664da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed566c1bdeaa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8d42be0b06a7182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:48:36.734310Z",
     "start_time": "2023-11-11T08:48:36.552788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hims/Library/CloudStorage/GoogleDrive-anjalihimanshuojha@gmail.com/Other computers/My MacBook Air/sjsu/bigdata_tech-228/project/customer_segmentation/code/data_prep\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d326d27e4c7b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e1c16955dacb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:05:03.943551Z",
     "start_time": "2023-11-11T04:05:03.939759Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple_data = spark.sparkContext.parallelize([[1, \"Alice\", 50]]).toDF()\n",
    "# simple_data.count()\n",
    "# simple_data.first()\n",
    "# simple_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85234419e01b2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
