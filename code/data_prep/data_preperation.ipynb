{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:26:52.950192Z",
     "start_time": "2023-11-11T09:26:52.124663Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "baseInputPath = \"/Users/hims/Downloads/yelp_dataset/\"\n",
    "baseOutputPath = \"/tmp/test-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e10cc874d85d47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:26:58.242559Z",
     "start_time": "2023-11-11T09:26:54.750192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fdee0156cf8>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.0.0.39:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Project App</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_spark():\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Project App\")\n",
    "        .config(\"spark.executor.memory\", \"8000mb\")\n",
    "        .config('spark.sql.shuffle.partitions', 4)\n",
    "        .config('spark.default.parallelism', 4)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf465d677ba4db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:27:01.718249Z",
     "start_time": "2023-11-11T09:26:58.263049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n",
      "|             address|          attributes|         business_id|          categories|         city|               hours|is_open|  latitude|   longitude|                name|postal_code|review_count|stars|state|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n",
      "|1616 Chapala St, ...|{ByAppointmentOnl...|Pns2l4eNsfO8kk83d...|[Doctors, Traditi...|Santa Barbara|                null|      0|34.4266787|-119.7111968|Abby Rappoport, L...|      93101|           7|  5.0|   CA|\n",
      "|87 Grasso Plaza S...|{BusinessAcceptsC...|mpf3x-BjTdTEA3yCZ...|[Shipping Centers...|       Affton|{Monday -> 0:0-0:...|      1| 38.551126|  -90.335695|       The UPS Store|      63123|          15|  3.0|   MO|\n",
      "|5255 E Broadway Blvd|{BikeParking -> T...|tUFrWirKiKi_TAnsV...|[Department Store...|       Tucson|{Monday -> 8:0-22...|      0| 32.223236| -110.880452|              Target|      85711|          22|  3.5|   AZ|\n",
      "|         935 Race St|{RestaurantsDeliv...|MTSW4McQd7CbVtyjq...|[Restaurants, Foo...| Philadelphia|{Monday -> 7:0-20...|      1|39.9555052| -75.1555641|  St Honore Pastries|      19107|          80|  4.0|   PA|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------+----------+------------+--------------------+-----------+------------+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "def process_business_data(spark):\n",
    "    try:\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"hours\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"is_open\", LongType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"review_count\", LongType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ])\n",
    "        \n",
    "        businessDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_business.json', schema) \\\n",
    "        .withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "        businessDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/business\")\n",
    "        businessDf = spark.read.parquet(f\"{baseOutputPath}/business\")\n",
    "    businessDf.show(4)\n",
    "    return businessDf\n",
    "\n",
    "business_df = process_business_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca71d457191f09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:27:03.791016Z",
     "start_time": "2023-11-11T09:27:03.311584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+-----+--------------------+----+-----+------+------------+------+--------------------+-------------------+\n",
      "|average_stars|compliment_cool|compliment_cute|compliment_funny|compliment_hot|compliment_list|compliment_more|compliment_note|compliment_photos|compliment_plain|compliment_profile|compliment_writer| cool|               elite|fans|funny|  name|review_count|useful|             user_id|      yelping_since|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+-----+--------------------+----+-----+------+------------+------+--------------------+-------------------+\n",
      "|         3.91|            467|             56|             467|           250|             18|             65|            232|              180|             844|                55|              239| 5994|              [2007]| 267| 1259|Walker|         585|  7217|qVc8ODYU5SZjKXVBg...|2007-01-25 16:47:26|\n",
      "|         3.74|           3131|            157|            3131|          1145|            251|            264|           1847|             1946|            7054|               184|             1521|27281|[2009,2010,2011,2...|3138|13066|Daniel|        4333| 43091|j14WgRoU_-2ZE1aw1...|2009-01-25 04:35:42|\n",
      "|         3.32|            119|             17|             119|            89|              3|             13|             66|               18|              96|                10|               35| 1003|[2009,2010,2011,2...|  52| 1010| Steph|         665|  2086|2WnXYQFK0hXEoTxPt...|2008-07-25 10:41:00|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+-----+--------------------+----+-----+------+------------+------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_user_data(spark):\n",
    "    try:\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "    except Exception as e:\n",
    "        userDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .drop(\"friends\") \\\n",
    "            .withColumn(\"elite\", split(col(\"elite\"), \", \")) \\\n",
    "            .withColumn(\"yelping_since\", col(\"yelping_since\").cast(\"timestamp\"))\n",
    "        \n",
    "        userDf.printSchema()\n",
    "        userDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/user\")\n",
    "        userDf = spark.read.parquet(f\"{baseOutputPath}/user\")\n",
    "        \n",
    "    userDf.show(3)\n",
    "    return userDf\n",
    "\n",
    "user_df = process_user_data(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb936e37caffac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:27:04.433235Z",
     "start_time": "2023-11-11T09:27:03.952574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|             friends|\n",
      "+--------------------+--------------------+\n",
      "|qVc8ODYU5SZjKXVBg...|[NSCy54eWehBJyZdG...|\n",
      "|j14WgRoU_-2ZE1aw1...|[ueRPE0CX75ePGMqO...|\n",
      "|2WnXYQFK0hXEoTxPt...|[LuO3Bn4f3rlhyHIa...|\n",
      "|SZDeASXq7o05mMNLs...|[enx1vVPnfdNUdPho...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_friends_data(spark):\n",
    "    try:\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    except Exception as e:\n",
    "        friendsDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \\\n",
    "            .select(\"user_id\", split(col(\"friends\"), \", \").alias(\"friends\"))\n",
    "        \n",
    "        friendsDf.printSchema()\n",
    "        friendsDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/friends\")\n",
    "        friendsDf = spark.read.parquet(f\"{baseOutputPath}/friends\")\n",
    "    \n",
    "    friendsDf.show(4)\n",
    "    return friendsDf\n",
    "\n",
    "friends_df = process_friends_data(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1637d8718a36c1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:27:05.665930Z",
     "start_time": "2023-11-11T09:27:05.334931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         business_id|                date|\n",
      "+--------------------+--------------------+\n",
      "|---kPU91CF4Lq2-Wl...|[2020-03-13 21:10...|\n",
      "|--0iUa4sNDFiZFrAd...|[2010-09-13 21:43...|\n",
      "|--30_8IhuyMHbSOcN...|[2013-06-14 23:29...|\n",
      "|--7PUidqRWpRSpXeb...|[2011-02-15 17:12...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_checkin_data(spark):\n",
    "    try:\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "    except Exception as e:\n",
    "        checkinDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_checkin.json') \\\n",
    "            .withColumn(\"date\", expr(\"transform(split(date, ', '), d -> to_timestamp(d))\").cast(ArrayType(TimestampType())))\n",
    "\n",
    "        checkinDf.printSchema()\n",
    "\n",
    "        checkinDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/checkin\")\n",
    "        checkinDf = spark.read.parquet(f\"{baseOutputPath}/checkin\")\n",
    "\n",
    "    checkinDf.show(4)\n",
    "    return checkinDf\n",
    "\n",
    "checkin_df = process_checkin_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef6e5e012af697a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:27:06.919076Z",
     "start_time": "2023-11-11T09:27:06.624887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+-------------------+---------------------------------------------------------+----------------------+\n",
      "|business_id           |compliment_count|date               |text                                                     |user_id               |\n",
      "+----------------------+----------------+-------------------+---------------------------------------------------------+----------------------+\n",
      "|3uLgwr0qeCNMjKenHJwPGQ|0               |2012-05-18 02:17:21|Avengers time with the ladies.                           |AGNUgVwnZUey3gcPCJ76iw|\n",
      "|QoezRbYQncpRqyrLH6Iqjg|0               |2013-02-05 18:35:10|They have lots of good deserts and tasty cuban sandwiches|NBN4MgHP9D3cw--SnauTkA|\n",
      "|MYoRNLb5chwjQe3c_k37Gg|0               |2013-08-18 00:56:08|It's open even when you think it isn't                   |-copOvldyKh1qr-vzkDEvw|\n",
      "|hV-bABTK-glh5wj31ps_Jw|0               |2017-06-27 23:05:38|Very decent fried chicken                                |FjMQVZjSqY8syIO-53KFKw|\n",
      "+----------------------+----------------+-------------------+---------------------------------------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_tip_data(spark):\n",
    "    try:\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    except Exception as e:\n",
    "        tipDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_tip.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "        \n",
    "        tipDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/tip\")\n",
    "        tipDf = spark.read.parquet(f\"{baseOutputPath}/tip\")\n",
    "    \n",
    "    tipDf.show(4, truncate=False)\n",
    "    return tipDf\n",
    "\n",
    "tip_df = process_tip_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5154ddb877eedf0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8446c4c1b593419"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9caea94eac02882"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "78fde7358d09b82e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "731d3807e75fae4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ca10da0a09facc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7e723384fb3d0459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e550c51a5bf5ba7b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6256d6f660533e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T10:04:23.664427Z",
     "start_time": "2023-11-11T09:28:09.351621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hims/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- frequent_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+---------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|sentiment|      frequent_words|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+---------+--------------------+\n",
      "|grpNey31cTGKrhmQQ...|   0|2020-06-28 19:45:32|    0|6fObpwIggOQR1oDap...|  5.0|Had a wonderful, ...|     0|lN-1uUHeV_QyFbczw...| positive|[wonderful, authe...|\n",
      "|Fay6yoOC6iitEt3QL...|   0|2020-06-07 18:13:33|    0|UgtrUhfuEgUdPay75...|  4.0|Yeah it was defin...|     0|TJ8Hawan8jDIZHS7A...| positive|[definitely, chee...|\n",
      "|wQq0QBaYXa1KLNw_J...|   0|2018-08-17 18:51:47|    0|MmLxg9oLQmPpcPNqI...|  4.0|One of the last s...|     1|ML10yeoSaW60TwVaI...| positive|[poker, run, one,...|\n",
      "|xgJMQq0uVY4KB9Efn...|   0|2020-06-15 14:31:14|    0|awQEOCuJ9fL12h7iq...|  5.0|Great experience ...|     0|w3z001eXLTQrYAIFe...| positive|[great, experienc...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+---------+--------------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def tokenize_and_get_top_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "    return top_words\n",
    "\n",
    "def process_review_data(spark):\n",
    "    try:\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "    except Exception as e:\n",
    "        reviewDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json') \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "            .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "        reviewDf.printSchema()\n",
    "        reviewDf.write.mode(\"overwrite\").parquet(f\"{baseOutputPath}/review\")\n",
    "        reviewDf = spark.read.parquet(f\"{baseOutputPath}/review\")\n",
    "        \n",
    "    reviewDf.show(4)\n",
    "    return reviewDf\n",
    "\n",
    "review_df = process_review_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c07db030799225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:44:58.669566Z",
     "start_time": "2023-11-11T07:44:58.664799Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hims/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# \n",
    "# import nltk\n",
    "# \n",
    "# nltk.download('vader_lexicon')\n",
    "# \n",
    "# def get_sentiments_df(df):\n",
    "#     # Initialize the Sentiment Intensity Analyzer\n",
    "#     sia = SentimentIntensityAnalyzer()\n",
    "#     # Define a UDF for sentiment analysis\n",
    "#     def get_sentiment(text):\n",
    "#         sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "#         if sentiment_score >= 0.05:\n",
    "#             return \"positive\"\n",
    "#         elif sentiment_score <= -0.05:\n",
    "#             return \"negative\"\n",
    "#         else:\n",
    "#             return \"neutral\"\n",
    "#     \n",
    "#     sentiment_udf = udf(get_sentiment, StringType())\n",
    "#     df = review_df.select(\"user_id\", \"text\", sentiment_udf(col(\"text\")).alias(\"sentiment\"))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             user_id|                text|      frequent_words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Hi10sGSZNxQH3NLyW...|Even if you come ...|[even, come, ming...|\n",
      "|jpx_StWgnkrTwC_eI...|On the Tuesday be...|[holiday, metropo...|\n",
      "|rZZKSzzMkGawlbY9G...|My favorite Itali...|[favorite, italia...|\n",
      "|ZPDnkhw4FcLHyNp1V...|Cut my workout sh...|[heather, machine...|\n",
      "|tMEdZaMYP0kOaCBVG...|Stopped by on my ...|[spicy, def, leve...|\n",
      "|4hEAu_7w9mbMhwarY...|From the very beg...|[move, loud, woul...|\n",
      "|5mY1UAGGO2p-4V1Pd...|Wolfie's is great...|[wolfie, great, w...|\n",
      "|WCk1trU2NGjd_4TGB...|We've been seeing...|[seeing, orris, w...|\n",
      "|2Wuzmn21bSGNuLURQ...|We don't visit of...|[visit, often, do...|\n",
      "|rJjABF23pHPvXr8v8...|I love this resta...|[dinner, pork, lu...|\n",
      "|1CndurKBoAOdIlkZw...|I was in last nig...|[got, well, frien...|\n",
      "|Qjd7aIiKqnd68IfaR...|Disclaimer: I AM ...|[shop, neighborho...|\n",
      "|7ziWZULyiZv2TesYN...|so yummy and crea...|[flavors, santa, ...|\n",
      "|KPt2FJerntp3hroOB...|Recently moved to...|[place, diner, gr...|\n",
      "|HFTf8k0Hw1URaMHbl...|Wow you thought R...|[wow, thought, ri...|\n",
      "|79qKF8Cuu0sUUyq6A...|Outstanding frien...|[outstanding, fri...|\n",
      "|Dh6OTRg1ajZ6QLZgb...|This place has be...|[pizza, good, one...|\n",
      "|zJJdSa2VZL3KSGNJj...|Nice little hole ...|[nice, little, ho...|\n",
      "|MtDsCRx8tRZTN8V6Y...|It is pretty much...|[seafood, santa, ...|\n",
      "|lK7bEhOIdnQWWSvop...|I love that this ...|[love, fantastic,...|\n",
      "+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# # from nltk.tokenize import word_tokenize\n",
    "# # from nltk.probability import FreqDist\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from pyspark.sql.types import StringType, ArrayType, IntegerType, MapType\n",
    "# # from pyspark.sql.functions import col, udf, concat_ws, collect_list\n",
    "# \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# \n",
    "# def get_frequent_words():\n",
    "# \n",
    "#     @udf(ArrayType(StringType()))\n",
    "#     # @udf(MapType(StringType(), IntegerType()))\n",
    "#     def tokenize_and_get_top_words(text):\n",
    "# \n",
    "#         tokens = word_tokenize(text)\n",
    "#         tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in stop_words]    \n",
    "#         freq_dist = FreqDist(tokens)\n",
    "#         top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "#         print(top_words)\n",
    "#         return top_words\n",
    "# \n",
    "#     df = review_df.sample(.0001).select(\"user_id\", \"text\") \\\n",
    "#         .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "# \n",
    "#     # .groupBy(\"user_id\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"texts\")) \\\n",
    "# \n",
    "#     return df\n",
    "# \n",
    "# get_frequent_words().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:12:35.315943Z",
     "start_time": "2023-11-11T09:12:34.540847Z"
    }
   },
   "id": "a4e1d3b6f534d712"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "765"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.sample(.0001).count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T08:59:35.596994Z",
     "start_time": "2023-11-11T08:59:35.054023Z"
    }
   },
   "id": "abe9c2f99f3b7d4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "904c1bd61888af99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca406dc252d78c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21c91529e7d09427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d97bb2b9b3d664da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63ed566c1bdeaa53"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hims/Library/CloudStorage/GoogleDrive-anjalihimanshuojha@gmail.com/Other computers/My MacBook Air/sjsu/bigdata_tech-228/project/customer_segmentation/code/data_prep\r\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T08:48:36.734310Z",
     "start_time": "2023-11-11T08:48:36.552788Z"
    }
   },
   "id": "b8d42be0b06a7182"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d5d326d27e4c7b4d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# simple_data = spark.sparkContext.parallelize([[1, \"Alice\", 50]]).toDF()\n",
    "# simple_data.count()\n",
    "# simple_data.first()\n",
    "# simple_data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T04:05:03.943551Z",
     "start_time": "2023-11-11T04:05:03.939759Z"
    }
   },
   "id": "96e1c16955dacb15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "85234419e01b2c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
