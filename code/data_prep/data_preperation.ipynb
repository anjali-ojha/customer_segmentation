{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78be37067ea828e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:28:04.553756Z",
     "start_time": "2023-11-11T23:28:04.514383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configs.py\n"
     ]
    }
   ],
   "source": [
    "%run ../storage/snowflake.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:03.709756Z",
     "start_time": "2023-11-11T23:16:03.700884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fdc00bb79e8>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.0.0.39:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Project App</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType, StringType, MapType, ArrayType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import explode, create_map\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.types import IntegerType, MapType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "sample = 0.001\n",
    "baseInputPath = baseInputPath\n",
    "sampleOutputPath = f\"{baseOutputPath}/sample={sample}/\"\n",
    "\n",
    "spark = spark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Most Active Users"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f937b3e0f6cbcb23"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_sampled_users_data():\n",
    "    if sample != 1:\n",
    "        \n",
    "        try:\n",
    "            sampled_users = spark.read.parquet(f\"{sampleOutputPath}/sampled_user_id\")\n",
    "        except Exception as e:\n",
    "            sampled_users =  spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json') \\\n",
    "                                .groupBy(\"user_id\").count().orderBy(col(\"count\").desc()).select(\"user_id\").sample(sample)\n",
    "    \n",
    "            sampled_users.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/sampled_user_id\")\n",
    "            sampled_users = spark.read.parquet(f\"{sampleOutputPath}/sampled_user_id\")\n",
    "            \n",
    "        return sampled_users, True\n",
    "    else:\n",
    "        return None, False \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:15.248737Z",
     "start_time": "2023-11-11T23:16:15.240720Z"
    }
   },
   "id": "2411094334589b56"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_sampled_business_data():\n",
    "    if sample != 1:\n",
    "        try:\n",
    "            sampled_business = spark.read.parquet(f\"{sampleOutputPath}/sampled_business_id\")\n",
    "        except Exception as e:\n",
    "\n",
    "            sampled_user, _ = get_sampled_users_data()\n",
    "            sampled_business =  spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json') \\\n",
    "                                .join(sampled_user, on = [\"user_id\"]) \\\n",
    "                                .select(\"business_id\").distinct()\n",
    "\n",
    "            sampled_business.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/sampled_business_id\")\n",
    "            sampled_business = spark.read.parquet(f\"{sampleOutputPath}/sampled_business_id\")\n",
    "\n",
    "        return sampled_business, True\n",
    "    else:\n",
    "        return None, False\n",
    "\n",
    "# get_sampled_business_data()[0].show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:17.884942Z",
     "start_time": "2023-11-11T23:16:17.873492Z"
    }
   },
   "id": "e11074bb269a6919"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Users Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "545672e53a9378fa"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------------+----+-----+--------+------------+------+-------------------+\n",
      "|             user_id|average_stars|compliment_cool|compliment_cute|compliment_funny|compliment_hot|compliment_list|compliment_more|compliment_note|compliment_photos|compliment_plain|compliment_profile|compliment_writer|cool|               elite|fans|funny|    name|review_count|useful|      yelping_since|\n",
      "+--------------------+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------------+----+-----+--------+------------+------+-------------------+\n",
      "|CDjeQhhH7ZoSKBDQ5...|         3.89|             79|              0|              79|            21|              1|             10|             36|                7|             119|                 6|               19| 544|[2009,2010,2011,2...|  39|  233|    Dave|         391|  1174|2006-08-15 01:27:52|\n",
      "|qp7ENnWv8O43l4yhq...|         3.91|             17|              0|              17|             5|              1|              3|              4|                0|               4|                 0|                6|  91|              [2009]|   5|   54|  Elijah|         199|   179|2008-02-06 03:55:31|\n",
      "|kZnSGYEx0mLVGroGt...|         4.15|             60|              6|              60|            34|             12|              5|             66|               32|              34|                 2|               10| 904|         [2010,2011]|  54|  584|Ms. Edna|         772|  1394|2009-05-30 19:32:30|\n",
      "+--------------------+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------------+----+-----+--------+------------+------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_user_data(spark):\n",
    "    try:\n",
    "        userDf = spark.read.parquet(f\"{sampleOutputPath}/user\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        sampled_users, is_sampled = get_sampled_users_data()\n",
    "        userDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json') \n",
    "        if is_sampled:\n",
    "            userDf = userDf.join(sampled_users, on = [\"user_id\"])         \n",
    "        \n",
    "        userDf = userDf \\\n",
    "            .drop(\"friends\") \\\n",
    "            .withColumn(\"elite\", split(col(\"elite\"), \", \")) \\\n",
    "            .withColumn(\"yelping_since\", col(\"yelping_since\").cast(\"timestamp\"))\n",
    "\n",
    "        userDf.coalesce(1).write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/user\")\n",
    "        userDf = spark.read.parquet(f\"{sampleOutputPath}/user\")\n",
    "\n",
    "    return userDf\n",
    "\n",
    "user_df = process_user_data(spark)\n",
    "# user_df.show(3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:21:59.935436Z",
     "start_time": "2023-11-11T23:21:59.708774Z"
    }
   },
   "id": "ae85c651a1f8c6a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Business Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a7ea6ab8f14884a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf465d677ba4db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:32.539199Z",
     "start_time": "2023-11-11T23:16:32.425467Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_business_data(spark):\n",
    "    try:\n",
    "        businessDf = spark.read.parquet(f\"{sampleOutputPath}/business\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"hours\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"is_open\", LongType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"review_count\", LongType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "        sampled_business, is_sampled = get_sampled_business_data()\n",
    "        businessDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_business.json', schema)\n",
    "        if is_sampled:\n",
    "            businessDf = businessDf.join(sampled_business, on = [\"business_id\"])\n",
    "\n",
    "        \n",
    "        businessDf =  businessDf\\\n",
    "            .withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "        businessDf.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/business\")\n",
    "        businessDf = spark.read.parquet(f\"{sampleOutputPath}/business\")\n",
    "        \n",
    "    return businessDf\n",
    "\n",
    "business_df = process_business_data(spark)\n",
    "# business_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca71d457191f09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:34.254259Z",
     "start_time": "2023-11-11T23:16:34.250722Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb936e37caffac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:41.726467Z",
     "start_time": "2023-11-11T23:16:41.655613Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_friends_data(spark):\n",
    "    try:\n",
    "        friendsDf = spark.read.parquet(f\"{sampleOutputPath}/friends\")\n",
    "    except Exception as e:\n",
    "\n",
    "        sampled_users, is_sampled = get_sampled_users_data()\n",
    "        friendsDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json')\n",
    "        if is_sampled:\n",
    "            friendsDf = friendsDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "\n",
    "        friendsDf = friendsDf.select(\"user_id\", split(col(\"friends\"), \", \").alias(\"friends\"))\n",
    "        \n",
    "        friendsDf.printSchema()\n",
    "        friendsDf.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/friends\")\n",
    "        friendsDf = spark.read.parquet(f\"{sampleOutputPath}/friends\")\n",
    "    \n",
    "    return friendsDf\n",
    "\n",
    "friends_df = process_friends_data(spark)\n",
    "# friends_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1637d8718a36c1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:43.132776Z",
     "start_time": "2023-11-11T23:16:42.908144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         business_id|                date|\n",
      "+--------------------+--------------------+\n",
      "|--sXnWH9Xm6_NvIjy...|[2011-06-08 19:26...|\n",
      "|--sgBOzb76sjOQ-Xh...|[2018-08-01 23:08...|\n",
      "|-0E7laYjwZxEAQPhF...|[2012-12-17 16:18...|\n",
      "|-0Ym1Wg3bXd_TDz8J...|[2018-06-09 18:52...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "def process_checkin_data(spark):\n",
    "    try:\n",
    "        checkinDf = spark.read.parquet(f\"{sampleOutputPath}/checkin\")\n",
    "    except Exception as e:\n",
    "\n",
    "        sampled_business, is_sampled = get_sampled_business_data()\n",
    "        checkinDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_checkin.json')\n",
    "        if is_sampled:\n",
    "            checkinDf = checkinDf.join(sampled_business, on = [\"business_id\"])\n",
    "\n",
    "        checkinDf = checkinDf \\\n",
    "            .withColumn(\"date\", expr(\"transform(split(date, ', '), d -> to_timestamp(d))\").cast(ArrayType(TimestampType())))\n",
    "\n",
    "        checkinDf.printSchema()\n",
    "\n",
    "        checkinDf.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/checkin\")\n",
    "        checkinDf = spark.read.parquet(f\"{sampleOutputPath}/checkin\")\n",
    "\n",
    "    return checkinDf\n",
    "\n",
    "checkin_df = process_checkin_data(spark)\n",
    "# checkin_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef6e5e012af697a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:51.075894Z",
     "start_time": "2023-11-11T23:16:51.016621Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tip_data(spark):\n",
    "    try:\n",
    "        tipDf = spark.read.parquet(f\"{sampleOutputPath}/tip\")\n",
    "    except Exception as e:\n",
    "\n",
    "        sampled_users, is_sampled = get_sampled_users_data()\n",
    "        tipDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_tip.json')\n",
    "        if is_sampled:\n",
    "            tipDf = tipDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "\n",
    "        tipDf = tipDf.withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "        \n",
    "        tipDf.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/tip\")\n",
    "        tipDf = spark.read.parquet(f\"{sampleOutputPath}/tip\")\n",
    "    \n",
    "    return tipDf\n",
    "\n",
    "tip_df = process_tip_data(spark)\n",
    "# tip_df.show(4, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:53.053011Z",
     "start_time": "2023-11-11T23:16:53.041419Z"
    }
   },
   "id": "41b1d37b50a05e7d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:53.326825Z",
     "start_time": "2023-11-11T23:16:53.322410Z"
    }
   },
   "id": "e5ccb961ad14c25b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:16:53.583710Z",
     "start_time": "2023-11-11T23:16:53.573210Z"
    }
   },
   "id": "254c01453ff390a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Review Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f25c54183a49388e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6256d6f660533e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:17:09.692226Z",
     "start_time": "2023-11-11T23:16:56.250708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hims/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- frequent_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def tokenize_and_get_top_words(text, sample_size=0.0001):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "    return top_words\n",
    "\n",
    "def process_review_data(spark):\n",
    "    try:\n",
    "        reviewDf = spark.read.parquet(f\"{sampleOutputPath}/review\")\n",
    "    except Exception as e:\n",
    "        sampled_users, is_sampled = get_sampled_users_data()\n",
    "        reviewDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json')\n",
    "        if is_sampled:\n",
    "            reviewDf = reviewDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "\n",
    "        reviewDf = reviewDf \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "            .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "        reviewDf.printSchema()\n",
    "        reviewDf.write.mode(\"overwrite\").parquet(f\"{sampleOutputPath}/review\")\n",
    "        reviewDf = spark.read.parquet(f\"{sampleOutputPath}/review\")\n",
    "        \n",
    "    return reviewDf\n",
    "\n",
    "review_df = process_review_data(spark)\n",
    "# review_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "7681"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T23:17:26.599188Z",
     "start_time": "2023-11-11T23:17:26.406820Z"
    }
   },
   "id": "f5ad0d0732e5640d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c07db030799225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:06.696872Z",
     "start_time": "2023-11-11T12:26:06.691625Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# \n",
    "# import nltk\n",
    "# \n",
    "# nltk.download('vader_lexicon')\n",
    "# \n",
    "# def get_sentiments_df(df):\n",
    "#     # Initialize the Sentiment Intensity Analyzer\n",
    "#     sia = SentimentIntensityAnalyzer()\n",
    "#     # Define a UDF for sentiment analysis\n",
    "#     def get_sentiment(text):\n",
    "#         sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "#         if sentiment_score >= 0.05:\n",
    "#             return \"positive\"\n",
    "#         elif sentiment_score <= -0.05:\n",
    "#             return \"negative\"\n",
    "#         else:\n",
    "#             return \"neutral\"\n",
    "#     \n",
    "#     sentiment_udf = udf(get_sentiment, StringType())\n",
    "#     df = review_df.select(\"user_id\", \"text\", sentiment_udf(col(\"text\")).alias(\"sentiment\"))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e1d3b6f534d712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:08.942513Z",
     "start_time": "2023-11-11T12:26:08.937555Z"
    }
   },
   "outputs": [],
   "source": [
    "# # from nltk.tokenize import word_tokenize\n",
    "# # from nltk.probability import FreqDist\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from pyspark.sql.types import StringType, ArrayType, IntegerType, MapType\n",
    "# # from pyspark.sql.functions import col, udf, concat_ws, collect_list\n",
    "# \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# \n",
    "# def get_frequent_words():\n",
    "# \n",
    "#     @udf(ArrayType(StringType()))\n",
    "#     # @udf(MapType(StringType(), IntegerType()))\n",
    "#     def tokenize_and_get_top_words(text):\n",
    "# \n",
    "#         tokens = word_tokenize(text)\n",
    "#         tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in stop_words]    \n",
    "#         freq_dist = FreqDist(tokens)\n",
    "#         top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "#         print(top_words)\n",
    "#         return top_words\n",
    "# \n",
    "#     df = review_df.sample(.0001).select(\"user_id\", \"text\") \\\n",
    "#         .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "# \n",
    "#     # .groupBy(\"user_id\").agg(concat_ws(\" \", collect_list(col(\"text\"))).alias(\"texts\")) \\\n",
    "# \n",
    "#     return df\n",
    "# \n",
    "# get_frequent_words().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe9c2f99f3b7d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T12:26:12.545736Z",
     "start_time": "2023-11-11T12:26:12.536426Z"
    }
   },
   "outputs": [],
   "source": [
    "# review_df.sample(.0001).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c1bd61888af99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca406dc252d78c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c91529e7d09427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bb2b9b3d664da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed566c1bdeaa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d42be0b06a7182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:25:57.993915Z",
     "start_time": "2023-11-11T23:25:57.987817Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d326d27e4c7b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e1c16955dacb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:05:03.943551Z",
     "start_time": "2023-11-11T04:05:03.939759Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple_data = spark.sparkContext.parallelize([[1, \"Alice\", 50]]).toDF()\n",
    "# simple_data.count()\n",
    "# simple_data.first()\n",
    "# simple_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85234419e01b2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
