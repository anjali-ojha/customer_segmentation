{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbbee5564935805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T23:35:38.091417Z",
     "start_time": "2023-11-11T23:35:38.087633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configs.py\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading configs.py\")\n",
    "\n",
    "baseInputPath = \"/Users/hims/Downloads/yelp_dataset/\"\n",
    "baseOutputPath = \"/tmp/test-1\"\n",
    "sample = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af43f849984464e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T22:32:25.947786Z",
     "start_time": "2023-11-11T22:32:25.105453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a Spark session\n",
    "# Specify the paths to the Snowflake connector JARs\n",
    "snowflake_jdbc_jar = \"/Users/hims/Downloads/snowflake-jdbc-3.14.0.jar\"\n",
    "spark_snowflake_jar = \"/Users/hims/Downloads/spark-snowflake_2.12-2.12.0-spark_3.4.jar\"\n",
    "\n",
    "\n",
    "# Create a Spark session with the Snowflake connector JARs\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \",\".join([\n",
    "    '--packages net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4',\n",
    "    'net.snowflake:snowflake-jdbc:3.14.0 pyspark-shell'])\n",
    "\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Project App\") \\\n",
    "        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4,net.snowflake:snowflake-jdbc:3.14.0 pyspark-shell\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# .master('local[*]') \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"64\") \\\n",
    "\n",
    "# .config(\"spark.driver.memory\", \"1gb\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8000mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74de94793932c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T22:32:29.862433Z",
     "start_time": "2023-11-11T22:32:26.720103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 14:32:27 WARN Utils: Your hostname, mac.local resolves to a loopback address: 127.0.0.1; using 10.0.0.39 instead (on interface en0)\n",
      "23/11/11 14:32:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/hims/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/hims/.ivy2/jars\n",
      "net.snowflake#spark-snowflake_2.12 added as a dependency\n",
      "net.snowflake#snowflake-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-426bf60a-ca65-4d77-a7b4-8bf9d45d6acc;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound net.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 in central\n",
      "\tfound net.snowflake#snowflake-ingest-sdk;0.10.8 in central\n",
      "\tfound net.snowflake#snowflake-jdbc;3.14.0 in central\n",
      ":: resolution report :: resolve 92ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-ingest-sdk;0.10.8 from central in [default]\n",
      "\tnet.snowflake#snowflake-jdbc;3.14.0 from central in [default]\n",
      "\tnet.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tnet.snowflake#snowflake-jdbc;3.13.30 by [net.snowflake#snowflake-jdbc;3.14.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-426bf60a-ca65-4d77-a7b4-8bf9d45d6acc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "23/11/11 14:32:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fac985edcd0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.0.0.39:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Project App</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5f03187d7eecb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
